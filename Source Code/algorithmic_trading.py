# -*- coding: utf-8 -*-
"""Algorithmic Trading.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EsIPMneWpadteAQ0-sk2IHijf2HOCMR

# Unsupervised Learning Trading Strategy

* Loading SP500 stock prices data.
* Calculating different features and indicators on each stock.
* Aggrigating on monthly level and filter top 150 most liquid stocks.
* Calculating monthly returns for different time-horizons.
* Downloding Fama-French Factors and Calculating Rolling Factor Betas.
* For each month fitting a K-Means Clustering Algorithm to group similar assets * based on their features.
* For each month selecting assets based on the cluster and form a portfolio.
* based on Efficient Frontier max sharpe ratio optimization.
* Visualizing Portfolio returns and compare to SP500 returns.

* Install the necessary packages
"""

!pip install pandas numpy matplotlib statsmodels pandas_datareader yfinance scikit-learn PyPortfolioOpt pandas_ta PyPortfolioOpt

from statsmodels.regression.rolling import RollingOLS
import pandas_datareader.data as web
import matplotlib.pyplot as plt
import statsmodels.api as sm
import pandas as pd
import numpy as np
import datetime as dt
import yfinance as yf
import pandas_ta
import warnings
warnings.filterwarnings('ignore')

"""**1. [Download](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) S&P 500 Dataset**




"""

sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]
sp500['Symbol'] = sp500['Symbol'].str.replace('.', '-')
symbol_list = sp500['Symbol'].unique().tolist()

end_date = '2024-09-27'
start_date = pd.to_datetime(end_date) - pd.DateOffset(years=10)

df = yf.download(tickers=symbol_list, start=start_date, end=end_date).stack()
df.index.names = ['date', 'ticker']
df.columns = df.columns.str.lower()

df

"""**2. Calculate features and technical indicators for each stock.**
* Garman-Klass Volatility
* RSI
* Bollinger Bands
* ATR
* MACD
* Dollar Volume

\begin{equation}
\text{Garman-Klass Volatility} = \frac{(\ln(\text{High}) - \ln(\text{Low}))^2}{2} - (2\ln(2) - 1)(\ln(\text{Adj Close}) - \ln(\text{Open}))^2
\end{equation}
"""

df['garman_klass_vol'] = ( (np.log(df['high'])-np.log(df['low']) )**2)/2 - (2*np.log(2)-1) * ( (np.log(df['adj close'])-np.log(df['open']) )**2 )
df['rsi'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(x, length=20) )

df['bb_low'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 0])
df['bb_mid'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 1])
df['bb_high'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 2])

def computer_atr(stock_data):
    atr = pandas_ta.atr(
        high = stock_data['high'],
        low = stock_data['low'],
        close = stock_data['adj close'],
        length = 14
    )
    return atr.sub(atr.mean()).div(atr.std())

df['atr'] = df.groupby(level=1, group_keys=False).apply(computer_atr)

def computer_macd(close):
    macd = pandas_ta.macd(
        close = close,
        length=20).iloc[:, 0]
    return macd.sub(macd.mean()).div(macd.std())

df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(computer_macd)

df['dollar_volume'] = (df['adj close']*df['volume']) / 1e6
df

"""**3. Aggregate to monthly level and filter top 150 most liquid stocks for each month.**
* To reduce training time and experiment with features and strategies, I'm converting the business-daily data to month-end frequency.
"""

last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume', 'open', 'high', 'low', 'close']]

data= (
    pd.concat(
        [df.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),
         df.unstack()[last_cols].resample('M').last().stack('ticker')], axis=1)
).dropna()

data

"""* Calculate 5-year rolling average of dollar volume for each stocks before filtering.

"""

data['dollar_volume'] = (data.loc[:, 'dollar_volume'].unstack('ticker').rolling(5*12, min_periods=12).mean().stack())

data['dollar_vol_rank'] = (data.groupby('date')['dollar_volume'].rank(ascending=False))

data = data[data['dollar_vol_rank']<150].drop(['dollar_volume', 'dollar_vol_rank'], axis=1)

data

"""**4. Calculate Monthly Returns for different time horizons as features.**
* To capture time series dynamics that reflect, for example, momentum patterns, I compute historical returns using the method .pct_change(lag), that is, returns over various monthly periods as identified by lags.
"""

def calculate_returns(df):
  outlier_cutoff = 0.005
  lags = [1,2,3,6,9,12]

  for lag in lags:
    df[f'return_{lag}m'] = (df['adj close']
                          .pct_change(lag)
                          .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),
                                                  upper=x.quantile(1-outlier_cutoff)))
                          .add(1)
                          .pow(1/lag)
                          .sub(1))
  return df

data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna()
data

"""**5. [Download Fama-French Factors](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) and Calculate Rolling Factor Betas.**


* I'll introduce the Fama—French data to estimate the exposure of assets to common risk factors using linear regression.

* The five Fama—French factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models.

* I can access the historical factor returns using the pandas-datareader and estimate historical exposures using the RollingOLS rolling linear regression.
"""

factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',
                               'famafrench',
                               start='2010')[0].drop('RF', axis=1)

factor_data.index = factor_data.index.to_timestamp()

factor_data = factor_data.resample('M').last().div(100)

factor_data.index.name = 'date'

# Ensuring both indices are timezone-aware
factor_data.index = factor_data.index.tz_localize('UTC')
factor_data = factor_data.join(data['return_1m']).sort_index()

factor_data

#checking outputs with examples
factor_data.xs('AAPL', level=1).head()

factor_data.xs('MSFT', level=1).head()

"""

*   Filter out stocks with less than 10 months of data.

"""

observations = factor_data.groupby(level=1).size()

valid_stocks = observations[observations >= 10]

factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]

factor_data

"""* Calculate [Rolling Factor](https://www.statsmodels.org/dev/generated/statsmodels.regression.rolling.RollingOLS.html) Betas."""

betas = (factor_data.groupby(level=1,
                            group_keys=False)
         .apply(lambda x: RollingOLS(endog=x['return_1m'],
                                     exog=sm.add_constant(x.drop('return_1m', axis=1)),
                                     window=min(24, x.shape[0]),
                                     min_nobs=len(x.columns)+1)
         .fit(params_only=True)
         .params
         .drop('const', axis=1)))

betas

"""* Join the rolling factors data to the main features dataframe.

"""

factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']

data = (data.join(betas.groupby('ticker').shift()))

data.loc[:, factors] = data.groupby('ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))

data = data.drop('adj close', axis=1)

data = data.dropna()

data.info()

"""At this point I've to decide on what ML model and approach to use for predictions etc.

**6. For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.**

* K-Means Clustering
I can also initialize predefined centroids for each cluster based on your research.

* For visualization purpose I will initially rely on the "k-means" initialization.

* Then I'll pre-define my centroids for each cluster.
"""

from sklearn.cluster import KMeans

data = data.drop('cluster', axis=1)

def get_clusters(df):
    df['cluster'] = KMeans(n_clusters=4,
                           random_state=0,
                           init=initial_centroids).fit(df).labels_
    return df

data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)

data

def plot_clusters(data):

    cluster_0 = data[data['cluster']==0]
    cluster_1 = data[data['cluster']==1]
    cluster_2 = data[data['cluster']==2]
    cluster_3 = data[data['cluster']==3]

    plt.scatter(cluster_0.iloc[:,0] , cluster_0.iloc[:,6] , color = 'red', label='cluster 0')
    plt.scatter(cluster_1.iloc[:,0] , cluster_1.iloc[:,6] , color = 'green', label='cluster 1')
    plt.scatter(cluster_2.iloc[:,0] , cluster_2.iloc[:,6] , color = 'blue', label='cluster 2')
    plt.scatter(cluster_3.iloc[:,0] , cluster_3.iloc[:,6] , color = 'black', label='cluster 3')

    plt.legend()
    plt.show()
    return

plt.style.use('ggplot')

for i in data.index.get_level_values('date').unique().tolist():

    g = data.xs(i, level=0)

    plt.title(f'Date {i}')

    plot_clusters(g)

"""* Apply pre-defined centroids."""

target_rsi_values = [30, 45, 55, 70]

initial_centroids = np.zeros((len(target_rsi_values), 18))

initial_centroids[:, 6] = target_rsi_values

initial_centroids

"""**7. For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization.**
* First I'll filter only stocks corresponding to the cluster I choose based on my hypothesis.

* Momentum is persistent and my idea would be that stocks clustered around RSI 70 centroid should continue to outperform in the following month - thus I would select stocks corresponding to cluster 3.
"""

filtered_df = data[data['cluster']==3].copy()

filtered_df = filtered_df.reset_index(level=1)

filtered_df.index = filtered_df.index+pd.DateOffset(1)

filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])

dates = filtered_df.index.get_level_values('date').unique().tolist()

fixed_dates = {}

for d in dates:

    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()

fixed_dates

"""**Define portfolio optimization function**
* I'll define a function which optimizes portfolio weights using  [PyPortfolioOpt](https://pyportfolioopt.readthedocs.io/en/latest/GeneralEfficientFrontier.html) package and EfficientFrontier optimizer to maximize the sharpe ratio.

* To optimize the weights of a given portfolio I would need to supply last 1 year prices to the function.

* Apply signle stock weight bounds constraint for diversification (minimum half of equaly weight and maximum 10% of portfolio).
"""

from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns

def optimize_weights(prices, lower_bound=0):

    returns = expected_returns.mean_historical_return(prices=prices,
                                                      frequency=252)

    cov = risk_models.sample_cov(prices=prices,
                                 frequency=252)

    ef = EfficientFrontier(expected_returns=returns,
                           cov_matrix=cov,
                           weight_bounds=(lower_bound, .1),
                           solver='SCS')

    weights = ef.max_sharpe()

    return ef.clean_weights()

"""* Download Fresh Daily Prices Data only for short listed stocks."""

stocks = data.index.get_level_values('ticker').unique().tolist()

new_df = yf.download(tickers=stocks,
                     start=data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),
                     end=data.index.get_level_values('date').unique()[-1])

new_df

"""* Calculate daily returns for each stock which could land up in my portfolio.

* Then loop over each month start, select the stocks for the month and calculate their weights for the next month.

* If the maximum sharpe ratio optimization fails for a given month, apply equally-weighted weights.

* Calculated each day portfolio return.
"""

from sre_constants import SUCCESS
returns_dataframe = np.log(new_df['Adj Close']).diff()

portfolio_df = pd.DataFrame()

for start_date in fixed_dates.keys():
  try:
    end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')
    cols = fixed_dates[start_date]
    optimization_start_date = (pd.to_datetime(start_date)-pd.DateOffset(months=12)).date().strftime('%Y-%m-%d')
    optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d')
    optimization_df = new_df[optimization_start_date:optimization_end_date]['Adj Close'][cols]

    SUCCESS = False

    try:
      weights = optimize_weights(prices=optimization_df, lower_bound=round(1/(len(optimization_df.columns)*2),3))
      weights = pd.DataFrame(weights, index=pd.Series(0))
      SUCCESS = True

    except:
      print(f'Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')

    if SUCCESS==False:
      weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],
                                      index=optimization_df.columns.tolist(),
                                      columns=pd.Series(0)).T

    temp_df = returns_dataframe[start_date:end_date]
    temp_df.index = temp_df.index.date
    temp_df = temp_df.stack().reset_index()
    temp_df.columns = ['Date', 'ticker', 'return']

    # Reset index of weights and rename columns to match temp_df for merging
    weights = weights.stack().reset_index()
    weights.columns = ['dummy', 'ticker', 'weight']
    weights = weights.drop('dummy', axis=1)

    # Merge temp_df and weights on 'ticker'
    temp_df = temp_df.merge(weights, on='ticker')

    # Set 'Date' and 'ticker' as index to remove the unwanted column
    temp_df = temp_df.set_index(['Date', 'ticker'])

    temp_df.unstack().stack()

    temp_df['weighted_return'] = temp_df['return']*temp_df['weight']
    temp_df=temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy_return')
    portfolio_df = pd.concat([portfolio_df, temp_df])

  except Exception as e:
          print(e)

portfolio_df = portfolio_df.drop_duplicates()
portfolio_df

portfolio_df.plot()

"""**8. Visualize Portfolio returns and compare to SP500 returns.**"""

spy = yf.download(tickers='SPY',
                  start='2015-01-01',
                  end=dt.date.today())

spy_ret = np.log(spy[['Adj Close']]).diff().dropna().rename({'Adj Close':'SPY Buy & Hold'}, axis=1)

portfolio_df = portfolio_df.merge(spy_ret,
                                  left_index=True,
                                  right_index=True)

portfolio_df

import matplotlib.ticker as mtick

plt.style.use('ggplot')

portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1

portfolio_cumulative_return[:'2024-09-29'].plot(figsize=(16,6))

plt.title('Unsupervised Learning Trading Strategy Returns Over Time')

plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))

plt.ylabel('Return')

plt.show()

"""# Twitter Sentiment Investing Strategy

**1. Load Twitter Sentiment Data**
* Load the twitter sentiment dataset, set the index, calculat engagement ratio and filter out stocks with no significant twitter activity.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import yfinance as yf
import os

plt.style.use('ggplot')

data_folder = '/content/sentiment_data.csv'

sentiment_df = pd.read_csv(data_folder)

sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])

sentiment_df = sentiment_df.set_index(['date', 'symbol'])

sentiment_df['engagement_ratio'] = sentiment_df['twitterComments']/sentiment_df['twitterLikes']

sentiment_df = sentiment_df[(sentiment_df['twitterLikes']>20)&(sentiment_df['twitterComments']>10)]

sentiment_df

"""**2. Aggregate Monthly and calculate average sentiment for the month**
* Aggregate on a monthly level and calculate average monthly metric, for the one I choose.

"""

aggragated_df = (sentiment_df.reset_index('symbol').groupby([pd.Grouper(freq='M'), 'symbol'])
                    [['engagement_ratio']].mean())

aggragated_df['rank'] = (aggragated_df.groupby(level=0)['engagement_ratio']
                         .transform(lambda x: x.rank(ascending=False)))

aggragated_df

"""**3. Select Top 5 Stocks based on their cross-sectional ranking for each month**
* Select top 5 stocks by rank for each month and fix the date to start at beginning of next month.

"""

filtered_df = aggragated_df[aggragated_df['rank']<6].copy()

filtered_df = filtered_df.reset_index(level=1)

filtered_df.index = filtered_df.index+pd.DateOffset(1)

filtered_df = filtered_df.reset_index().set_index(['date', 'symbol'])

filtered_df.head(20)

"""**4. Extract the stocks to form portfolios with at the start of each new month**
* Create a dictionary containing start of month and corresponded selected stocks.

"""

dates = filtered_df.index.get_level_values('date').unique().tolist()

fixed_dates = {}

for d in dates:

    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()

fixed_dates

"""**5. Download fresh stock prices for only selected/shortlisted stocks**

"""

stocks_list = sentiment_df.index.get_level_values('symbol').unique().tolist()
print(stocks_list.remove('ATVI'))
prices_df = yf.download(tickers=stocks_list,
                        start='2021-01-01',
                        end='2023-03-01')

"""**6. Calculate Portfolio Returns with monthly rebalancing**

"""

returns_df = np.log(prices_df['Adj Close']).diff().dropna()

portfolio_df = pd.DataFrame()

for start_date in fixed_dates.keys():

    end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd()).strftime('%Y-%m-%d')

    cols = fixed_dates[start_date]

    temp_df = returns_df[start_date:end_date][cols].mean(axis=1).to_frame('portfolio_return')

    portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)

portfolio_df.index = portfolio_df.index.date
portfolio_df

"""**7. Download NASDAQ/QQQ prices and calculate returns to compare to my strategy**

"""

qqq_df = yf.download(tickers='QQQ',
                     start='2021-01-01',
                     end='2023-03-01')

qqq_ret = np.log(qqq_df['Adj Close']).diff().to_frame('nasdaq_return')

portfolio_df = portfolio_df.merge(qqq_ret,
                                  left_index=True,
                                  right_index=True)

portfolio_df

portfolios_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum()).sub(1)

portfolios_cumulative_return.plot(figsize=(16,6))

plt.title('Twitter Engagement Ratio Strategy Return Over Time')

plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))

plt.ylabel('Return')

plt.show()

"""# Intraday Strategy Using GARCH Model

* Using simulated daily data and intraday 5-min data.
* Load Daily and 5-minute data.
* Define function to fit GARCH model on the daily data and predict 1-day ahead volatility in a rolling window.
* Calculate prediction premium and form a daily signal from it.
* Merge with intraday data and calculate intraday indicators to form the intraday signal.
* Generate the position entry and hold until the end of the day.
* Calculate final strategy returns.

**1. Load Simulated Daily and Simulated 5-minute data.**
* I'm loading both datasets, set the indexes and calculate daily log returns.
"""

!pip install arch # install the arch module

from arch import arch_model

data_folder = '/content/simulated_daily_data.csv'

daily_df = pd.read_csv(data_folder)

daily_df = daily_df.drop('Unnamed: 7', axis=1)

daily_df['Date'] = pd.to_datetime(daily_df['Date'])

daily_df = daily_df.set_index('Date')

intraday_5min_df = pd.read_csv('/content/simulated_5min_data.csv')

intraday_5min_df = intraday_5min_df.drop('Unnamed: 6', axis=1)

intraday_5min_df['datetime'] = pd.to_datetime(intraday_5min_df['datetime'])

intraday_5min_df = intraday_5min_df.set_index('datetime')

intraday_5min_df['date'] = pd.to_datetime(intraday_5min_df.index.date)

intraday_5min_df

"""**2. Define function to fit GARCH model and predict 1-day ahead volatility in a rolling window.**
* I'll first calculate the 6-month rolling variance and then I'll create a function in a 6-month rolling window to fit a garch model and predict the next day variance.
"""

daily_df['log_ret'] = np.log(daily_df['Adj Close']).diff()

daily_df['variance'] = daily_df['log_ret'].rolling(180).var()

daily_df = daily_df['2020':]

def predict_volatility(x):

    best_model = arch_model(y=x,
                            p=1,
                            q=3).fit(update_freq=5,
                                     disp='off')

    variance_forecast = best_model.forecast(horizon=1).variance.iloc[-1,0]

    print(x.index[-1])

    return variance_forecast

daily_df['predictions'] = daily_df['log_ret'].rolling(180).apply(lambda x: predict_volatility(x))

daily_df = daily_df.dropna()

daily_df

"""**3. Calculate prediction premium and form a daily signal from it.**
* I'll calculate the prediction premium.
* calculate its 6-month rolling standard deviation.

* From this I'll create my daily signal.
"""

daily_df['prediction_premium'] = (daily_df['predictions']-daily_df['variance'])/daily_df['variance']

daily_df['premium_std'] = daily_df['prediction_premium'].rolling(180).std()

daily_df['signal_daily'] = daily_df.apply(lambda x: 1 if (x['prediction_premium']>x['premium_std'])
                                         else (-1 if (x['prediction_premium']<x['premium_std']*-1) else np.nan),
                                         axis=1)

daily_df['signal_daily'] = daily_df['signal_daily'].shift()

daily_df

"""**4. Merge with intraday data and calculate intraday indicators to form the** intraday signal.
* Calculate all intraday indicators and intraday signal.
"""

final_df = intraday_5min_df.reset_index()\
                            .merge(daily_df[['signal_daily']].reset_index(),
                                   left_on='date',
                                   right_on='Date')\
                            .drop(['date','Date'], axis=1)\
                            .set_index('datetime')

final_df['rsi'] = pandas_ta.rsi(close=final_df['close'],
                                length=20)

final_df['lband'] = pandas_ta.bbands(close=final_df['close'],
                                     length=20).iloc[:,0]

final_df['uband'] = pandas_ta.bbands(close=final_df['close'],
                                     length=20).iloc[:,2]

final_df['signal_intraday'] = final_df.apply(lambda x: 1 if (x['rsi']>70)&
                                                            (x['close']>x['uband'])
                                             else (-1 if (x['rsi']<30)&
                                                         (x['close']<x['lband']) else np.nan),
                                             axis=1)

final_df['return'] = np.log(final_df['close']).diff()

final_df

"""**5. Generate the position entry and hold until the end of the day.**

"""

final_df['return_sign'] = final_df.apply(lambda x: -1 if (x['signal_daily']==1)&(x['signal_intraday']==1)
                                        else (1 if (x['signal_daily']==-1)&(x['signal_intraday']==-1) else np.nan),
                                        axis=1)

final_df['return_sign'] = final_df.groupby(pd.Grouper(freq='D'))['return_sign']\
                                  .transform(lambda x: x.ffill())

final_df['forward_return'] = final_df['return'].shift(-1)

final_df['strategy_return'] = final_df['forward_return']*final_df['return_sign']

daily_return_df = final_df.groupby(pd.Grouper(freq='D'))['strategy_return'].sum()

"""**6. Calculate final strategy returns.**

"""

import matplotlib.ticker as mtick

strategy_cumulative_return = np.exp(np.log1p(daily_return_df).cumsum()).sub(1)

strategy_cumulative_return.plot(figsize=(16,6))

plt.title('Intraday Strategy Returns')

plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))

plt.ylabel('Return')

plt.show()
